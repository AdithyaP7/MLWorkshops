{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Student_WinnerWinnerChickenDinner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdithyaP7/MLWorkshops/blob/master/ReinforcementLearningBlackjack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el79IfPCPqri",
        "colab_type": "text"
      },
      "source": [
        "# The Game of Blackjack\n",
        "\n",
        "The most widely-played game of chance in casinos around the world is the card game Blackjack. Though you can be sure that the odds are *always* against the gambler, certain play strategies are better than others (in the sense that they provide the gambler with a smaller expected loss). Despite Blackjack having been played since the 17th century, it was not until 1956 that a team of statisticians managed to find the provably optimal play strategy through a lengthy series of probability calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHxrLdc2W80c",
        "colab_type": "text"
      },
      "source": [
        "![](https://upload.wikimedia.org/wikipedia/commons/3/33/Blackjack21.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRRzsFBwPC-6",
        "colab_type": "text"
      },
      "source": [
        "Here's a brief description of how the game works in its most basic form (you can find the full rules [here](https://en.wikipedia.org/wiki/Blackjack#Rules), but you don't need a detailed understanding for the purposes of this notebook). The general objective is to accumulate points, via playing cards, that sum as close to 21 as possible without exceeding it.\n",
        "\n",
        "Each card in the deck has a numeric value between 2 and 10, except for one type of card - an \"ace\" - which can be worth either 1 or 11. The value of a hand is given by the sum of the values of its cards. We (the gambler) are dealt two cards, both of which we can see. Our opponent (the dealer) is also dealt two cards, of which we can see only one.\n",
        "\n",
        "We are first given the option to draw additional cards - one by one, as many as we'd like - to try to improve our hand. This is called \"hitting\". If we exceed 21 as a result of hitting, we lose. Once we are done hitting (this is called \"sticking\"), it becomes the dealer's turn. The dealer reveals their hidden card and must hit until their hand has a value of at least 17. At this point, the dealer must stick. If the dealer exceeds 21, we win. Otherwise, the player (either us or the dealer) with the higher hand value at this point wins. If both players have the same hand value, the game ends in a tie.\n",
        "\n",
        "\\\\\n",
        "\n",
        "A Blackjack *strategy* consists of choosing whether to hit or stick based on the cards in the player's hand as well as the dealer's visible card. Using our understanding of the game mechanics, it shouldn't be too difficult to simulate games of Blackjack and try out different strategies! We may not be expert statisticians, but perhaps **reinforcement learning** - our framework for learning through trial-and-error - can do the heavy lifting for us in finding a strong Blackjack strategy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvrCQDm8POja",
        "colab_type": "text"
      },
      "source": [
        "## Blackjack as an MDP\n",
        "\n",
        "In order to apply reinforcement learning, we need to formulate Blackjack in terms of **states, actions, rewards,** and **transitions**. Take a second to think about how you might do this!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lu3XkyERndf",
        "colab_type": "text"
      },
      "source": [
        "The **state** of the game corresponds to the information that the agent has when making decisions. As described above, this consists only of the cards in the agent's hand, as well as the dealer's visible card. However, it turns out (with one exception) that we don't actually care about the specific cards in our hand - we just care about their sum! \n",
        "\n",
        "The exception is the ace; aces can be worth either 1 or 11, and this information is not captured by the sum alone. If the player holds an ace that could be used as 11 without the total hand value exceeding 21, then the ace is said to be *usable*.\n",
        "\n",
        "So we will represent a state using the following format:\n",
        "\n",
        "$s = (s^{(1)}, s^{(2)}, s^{(3)})$\n",
        "\n",
        "where:\n",
        "\n",
        "*   $s^{(1)} =$ sum of cards in hand (integer)\n",
        "*   $s^{(2)} =$ dealer's visible card (integer) \n",
        "*   $s^{(3)} =$ whether or not we have a useable ace (boolean)\n",
        "\n",
        "The **actions**, as described above, are to hit or to stick.\n",
        "\n",
        "Because we are interested in training our agent to win as much as possible, we'll define a simple **reward** function which is $+1$ upon winning, $-1$ upon losing, and $0$ at all intermediate (i.e. non-game-ending) states, as well as upon tying.\n",
        "\n",
        "The **transitions** are determined by the probabilities of drawing particular cards from the deck, together with the rules of the game. Luckily, we don't need to worry about implementing the mechanics of Blackjack ourselves; [OpenAI](https://openai.com/) has done it for us!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsZX9PS8ZI80",
        "colab_type": "text"
      },
      "source": [
        "# Hitting the Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkSmiO9Nh0ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run this to load some packages! { display-mode: \"form\" }\n",
        "import random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib import cm\n",
        "import gym\n",
        "\n",
        "def get_best_action(hand_val, dealer_card, usable_ace):\n",
        "    state = (hand_val, dealer_card, usable_ace)\n",
        "\n",
        "    # Compute Q(state, True)\n",
        "    if num_games[(state,True)] == 0:\n",
        "      Q_state_true = 0\n",
        "    else:\n",
        "      Q_state_true = sum_rewards[(state,True)] / num_games[(state,True)]\n",
        "    \n",
        "    # Compute Q(state, False)\n",
        "    if num_games[(state,False)] == 0:\n",
        "      Q_state_false = 0\n",
        "    else:\n",
        "      Q_state_false = sum_rewards[(state,False)] / num_games[(state,False)]\n",
        "\n",
        "    return Q_state_true >= Q_state_false\n",
        "\n",
        "def make_subplot(ax, usable_ace):\n",
        "    x_coords = np.arange(1, 11) # dealer's cards\n",
        "    y_coords = np.arange(11, 22) # gambler's hand values\n",
        "    Z = np.array([[get_best_action(y, x, usable_ace) for x in x_coords] for y in y_coords])\n",
        "    surf = ax.imshow(Z, cmap=plt.get_cmap('Set1', 2), extent=[0.5, 10.5, 21.5, 10.5])\n",
        "    plt.xticks(x_coords)\n",
        "    plt.yticks(y_coords)\n",
        "    plt.gca().invert_yaxis()\n",
        "    ax.set_xlabel('Dealer Showing')\n",
        "    ax.set_ylabel('Player Sum')\n",
        "    col1_patch = mpatches.Patch(color='darkgray', label='Hit')\n",
        "    col2_patch = mpatches.Patch(color='red', label='Stick')\n",
        "    plt.legend(handles=[col1_patch, col2_patch])\n",
        "\n",
        "def plot_strategy():\n",
        "  fig = plt.figure(figsize=(13, 13))\n",
        "  ax = fig.add_subplot(121)\n",
        "  ax.set_title('Usable Ace')\n",
        "  make_subplot(ax, True)\n",
        "  ax = fig.add_subplot(122)\n",
        "  ax.set_title('No Usable Ace')\n",
        "  make_subplot(ax, False)\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6N-gc3EZLqO",
        "colab_type": "text"
      },
      "source": [
        "We first create a Blackjack environment using OpenAI's Gym package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6oZJUwQYx51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blackjack = gym.make('Blackjack-v0')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6U1UGy5ZfWI",
        "colab_type": "text"
      },
      "source": [
        "Let's try playing a game of Blackjack in the gym environment by hand. We can run `blackjack.reset()` to start a new game - doing so gives us the initial game state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2aBEURIZfFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "14593e75-459b-4e08-c510-1e46c7843877"
      },
      "source": [
        "initial_state = blackjack.reset()\n",
        "\n",
        "print(\"Starting state:\", initial_state)\n",
        "print(\"Hand value:\", initial_state[0])\n",
        "print(\"Dealer's card:\", initial_state[1])\n",
        "print(\"Useable ace?:\", initial_state[2])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting state: (11, 10, False)\n",
            "Hand value: 11\n",
            "Dealer's card: 10\n",
            "Useable ace?: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StxtrlgqeI2k",
        "colab_type": "text"
      },
      "source": [
        "We'll use `True` to denote the \"hit\" action, and `False` to denote the \"stick\" action.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeMk_2aAeGgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hit = True\n",
        "stick = False"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ir-iRxga0OM",
        "colab_type": "text"
      },
      "source": [
        "We can take an action by running `blackjack.step(action)` -  doing so gives us back four values. We won't worry about the fourth, but the first three are: \n",
        "\n",
        "1.   The resulting state that we transition to upon taking the action\n",
        "2.   The reward that we get for this transition (`+1`, `-1`, or `0`)\n",
        "3.   Whether or not the game is over (`True` or `False`)\n",
        "\n",
        "Note that we will only see a reward of `+1` or `-1` when the game is over."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4y_rsFWbU8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "78daae8d-c423-4485-f899-11b408a1e24b"
      },
      "source": [
        "next_state, reward, game_over, _ = blackjack.step(hit)\n",
        "print(\"Action: Hit\")\n",
        "print(\"Resulting State:\", next_state)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Game over?:\", game_over)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action: Hit\n",
            "Resulting State: (20, 10, False)\n",
            "Reward: 0.0\n",
            "Game over?: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhhJqnC_f7Em",
        "colab_type": "text"
      },
      "source": [
        "If the game isn't over yet, we can continue taking actions in this same way. If the game is over, we can use `blackjack.reset()` again to start a new game!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-tPAx-3gflv",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1 ✍️\n",
        "\n",
        "Try seeing if you can win a game of Blackjack! You'll know you've won if you see a `reward` of `1.0`. You may find it helpful to use multiple code cells when playing by hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rTt4Q4agwYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3dafb76a-c68a-40a7-ddd7-e48ccb2345b6"
      },
      "source": [
        " initial_state = blackjack.reset()\n",
        "\n",
        "print(\"Hand value:\", initial_state[0])\n",
        "print(\"Dealer's card:\", initial_state[1])\n",
        "print(\"Useable ace?:\", initial_state[2])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hand value: 18\n",
            "Dealer's card: 4\n",
            "Useable ace?: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUpTE0VthL4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "515188fd-99af-4823-ba86-7674a11bf1d7"
      },
      "source": [
        "next_state, reward, game_over, _ = blackjack.step(hit)\n",
        "\n",
        "print(\"Hand value:\", next_state[0])\n",
        "print(\"Dealer's card:\", next_state[1])\n",
        "print(\"Useable ace?:\", next_state[2])\n",
        "print(\"-----\")\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Game over?:\", game_over)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hand value: 23\n",
            "Dealer's card: 3\n",
            "Useable ace?: False\n",
            "-----\n",
            "Reward: -1.0\n",
            "Game over?: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVBeJSW5hPsv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c97fcc25-46c9-4698-a031-655342580a7a"
      },
      "source": [
        "next_state, reward, game_over, _ = blackjack.step(stick)\n",
        "\n",
        "print(\"Hand value:\", next_state[0])\n",
        "print(\"Dealer's card:\", next_state[1])\n",
        "print(\"Useable ace?:\", next_state[2])\n",
        "print(\"-----\")\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Game over?:\", game_over)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hand value: 18\n",
            "Dealer's card: 4\n",
            "Useable ace?: False\n",
            "-----\n",
            "Reward: 1.0\n",
            "Game over?: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSk5nmDDufk5",
        "colab_type": "text"
      },
      "source": [
        "# Overview of our Approach\n",
        "\n",
        "As described in the slides, we'll use a *value estimator* $Q(s,a)$ to estimate \"how good\" it is to take action $a$ in state $s$. Once we learn $Q$, we can easily extract our strategy: from any given state, pick the action with the highest value!\n",
        "\n",
        "In order to learn $Q$, we'll repeat the following two steps:\n",
        "\n",
        "\n",
        "1.   Play a game of Blackjack.\n",
        "2.   Update the $Q(s,a)$ value, for each $(s,a)$ pair that occurred in the game, based on whether we won or lost.\n",
        "\n",
        "It will be helpful to decompose our code for this process into separate *functions*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhm_SMJBhDn-",
        "colab_type": "text"
      },
      "source": [
        "# Estimating Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a_NevEHzOkG",
        "colab_type": "text"
      },
      "source": [
        "## Counters\n",
        "\n",
        "We'll find a particular Python object called a `Counter` helpful for storing our value estimates. A `Counter` is a *mapping* (just like a mathematical function!) that associates Python objects with numbers. By default, it associates every object with the number `0`. In the following code, we create a `Counter` and update the numbers associated with the strings `\"hello\"` and `\"world\"` to equal `1` and `2`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBvbpJ7Eg8Bd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bff24e74-800b-41a8-d2eb-e96bc48b518d"
      },
      "source": [
        "my_counter = Counter()\n",
        "my_counter[\"hello\"] = 1\n",
        "my_counter[\"world\"] = 2\n",
        "print(my_counter)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'world': 2, 'hello': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA0ZBWF4hEd0",
        "colab_type": "text"
      },
      "source": [
        "From the printout, we see that `\"world\"` is mapped to `2`, and `\"hello\"` is mapped to `1`; it's not explicitly listed, but every other object is mapped to zero, as we verify below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OcDh9WLhaBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56d29741-a520-4cbf-ec5f-49cb0e4c2f56"
      },
      "source": [
        "print(my_counter[\"new_word\"])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImNYbV-Wh_4j",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2 ✍️\n",
        "\n",
        "Create a `Counter` that maps every letter of the alphabet to the number of students in your cohort whose first name begins with that letter. Then, use your `Counter` to compute the number of students whose names begin with either \"A\", \"M\", or \"S\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q55_GxtKjWtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0f6e876-c94a-48bb-8cf1-52723214d74c"
      },
      "source": [
        "my_counter = Counter()\n",
        "my_counter['A'] = 3\n",
        "my_counter['C'] = 1\n",
        "my_counter['M'] = 1\n",
        "my_counter['R'] = 2\n",
        "my_counter['Y'] = 1\n",
        "\n",
        "print(my_counter['A'] + my_counter['M'] + my_counter['S'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9x7OZsizQSC",
        "colab_type": "text"
      },
      "source": [
        "## Using Counters to store Value Estimates\n",
        "\n",
        "How might we use these `Counters` to store our value estimates? We want $Q(s,a)$ to equal the average, across all games where we took action $a$ in state $s$, of the end-game rewards from those games (`+1` for winning, `-1` for losing, `0` for tying). This average equals the sum of the end-game rewards across all of those games, divided by the number of games. So we need to track, for each $s,a$ pair:\n",
        "\n",
        "1.   Sum of end-game rewards across games where $s,a$ occurred.\n",
        "2.   Number of games where $s,a$ occurred.\n",
        "\n",
        "We will use a `Counter` called `sum_rewards` for the first of these quantities, and a `Counter` called `num_games` for the second. The objects that these `Counters` map will be `Tuples` of the form `(state,action)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nM9-rKulPdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_rewards = Counter()\n",
        "num_games = Counter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovvW4r8xzg-J",
        "colab_type": "text"
      },
      "source": [
        "## Updating the Value Estimates\n",
        "\n",
        "As described in the slides, our agent's experience over the course of a single game can be represented as:\n",
        "\n",
        "$s_1, a_1, r_1, s_2, a_2, r_2, \\ldots, s_n, a_n, r_n$\n",
        "\n",
        "Since we only receive nonzero rewards upon the game ending, we can more succinctly represent a game of Blackjack as:\n",
        "\n",
        "$s_1, a_1, s_2, a_2, \\ldots, s_n, a_n, r$\n",
        "\n",
        "Where $r$ is the reward received at the end of the game. One complication here is that Blackjack games last for a variable number of moves, so we don't know what $n$ is upfront!\n",
        "\n",
        "Suppose that, in code, this gameplay data is given you in the following form:\n",
        "\n",
        "`game = [(s_1,a_1), (s_2,a_2), ... , (s_n,a_n), r]`\n",
        "\n",
        "That is, `game` is a `List` of $n$ `Tuples` (each of which contains a state and an action) followed by a single reward at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMd4vJ9ynoXt",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3 ✍️\n",
        "\n",
        "Write a function that takes a `game` as argument and updates `sum_rewards` and `num_games` appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JfBDdE8n2EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_counters(game):\n",
        "  ### YOUR CODE HERE ###\n",
        "  reward = game[-1]\n",
        "  for indx in range(len(game)-1):\n",
        "    state_action_tuple = game[indx]\n",
        "    sum_rewards[state_action_tuple] += reward\n",
        "  sum_rewards = game.shape\n",
        "  num_rewards = game.shape\n",
        "  ### END CODE ###\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khzLKitdlkuW",
        "colab_type": "text"
      },
      "source": [
        "# Choosing Actions\n",
        "\n",
        "Now that we know how to update our value estimates from gameplay, it remains to generate gameplay from which to learn! Doing so will require us to choose actions; recall that we can do this using the $\\epsilon$-greedy approach to effectively balance **exploration** and **exploitation**.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBPHYmodFwfR",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4 ✍️\n",
        "\n",
        "Write a function that takes a `state` as argument and returns the best action based on your current value estimate.\n",
        "\n",
        "Recall that hitting is the best action in `state` if $Q$(`state`,`True`) $>$ $Q$(`state`, `False`), and sticking is the best action is if $Q$(`state`,`True`) $<$ $Q$(`state`, `False`). In the case that these two $Q$ values are equal, we'll be risky and say that the the best action is to hit. How can you compute these two $Q$ values?\n",
        "\n",
        "Hint: Be careful not to divide by zero! If you've never seen a `(state, action)` pair before, the pair should have a value of zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVaC3sy_GjU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_action(state):\n",
        "  ### YOUR CODE HERE ###\n",
        "  # Compute Q(state, True)\n",
        "  \n",
        "  \n",
        "  # Compute Q(state, False)\n",
        "  \n",
        "\n",
        "  return Q_state_true >= Q_state_false\n",
        "  ### END CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKGXAvTazfnR",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 5 ✍️\n",
        "\n",
        "Let's use our `best_action(state)` function to implement the $\\epsilon$-greedy action selection approach. Write a function that takes both a `state` and a number `epsilon` (between 0 and 1) as arguments and returns a random action with probability `epsilon` and the best action with probability `1 - epsilon`.\n",
        "\n",
        "Hint: You can generate a random boolean variable, which equals `True` with probability `prob` and equals `False` with probability $1 -$ `prob`, using the command `np.random.binomial(n=1,p=prob)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcIimTixzyGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epsilon_greedy(state, epsilon):\n",
        "  ### YOUR CODE HERE ###\n",
        "  \n",
        "\n",
        "  # Return best_action with probability 1-epsilon\n",
        "  \n",
        "  \n",
        "  # Otherwise, return random action\n",
        "  \n",
        "  ### END CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLG5qOLIgA0J",
        "colab_type": "text"
      },
      "source": [
        "# Playing Games\n",
        "\n",
        "Now let's use our function for $\\epsilon$-greedy action selection to actually play games of Blackjack!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng3315Dk6LF7",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 6 ✍️\n",
        "\n",
        "Write a function for playing a game of Blackjack (and returning the game data in the form of `game` as described above) where we choose actions using our `epsilon_greedy(state, epsilon)` function.\n",
        "\n",
        "Hint: this function should automate the gameplay logic that you executed manually in Exercise 1! In particular, you should continue querying your `epsilon_greedy(state, epsilon)` function to select actions, and using `blackjack.step(action)` to take the selected actions, until you see that `game_over == True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFIbWc2x58m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_blackjack(epsilon):\n",
        "  ### YOUR CODE HERE ###\n",
        "  game = []\n",
        "  state = blackjack.reset()\n",
        "  game_over = False\n",
        "  while game_over == False:\n",
        "      ### FINISH THE LOOP\n",
        "  return game\n",
        "  ### END CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LioUt8hnlfy3",
        "colab_type": "text"
      },
      "source": [
        "# Training our Agent\n",
        "\n",
        "Time to put it all together and actually train our agent to learn a good blackjack strategy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPkEPR2z44dp",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 7 ✍️\n",
        "\n",
        "Fill in the following `for` loop so that your agent plays a million games with the given `epsilon`, updating its `Counters` after each game. You should be able to do this with only two lines of code! \n",
        "\n",
        "Note that this code will take approximately 1 minute to finish running - a million games is a lot!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNB8AOHY7TtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.15\n",
        "for iteration in range(1000000):\n",
        "  ### YOUR CODE HERE ###\n",
        "\n",
        "  ### END CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwtdy2H_liFY",
        "colab_type": "text"
      },
      "source": [
        "# Strategy Visualization\n",
        "\n",
        "Let's take a look at the strategy our agent has learned! We've written a function called `plot_strategy()` that will do the visualization for you; give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu8_JP628kp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_strategy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_XiNWcl6n3G",
        "colab_type": "text"
      },
      "source": [
        "Compare the strategy that your agent learned to the optimal Blackjack strategy, shown below on the left. It should look pretty similar!\n",
        "\n",
        "(Notice that the grid layouts differ a bit between our visualization and the one below; we plot the best actions aligned with the tick marks, whereas the following figure plots the best actions in between tick marks).\n",
        "\n",
        "\n",
        "![alt text](https://ankonzoid.github.io/LearningX/classical_RL/blackjack/images/coverart.png)\n"
      ]
    }
  ]
}